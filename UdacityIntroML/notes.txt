# Udacity - Udacity Intro to Machine Learning

# Lesson Naive Bayes

sklearn naives bayes
http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html

```python
import numpy as np
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
Y = np.array([1, 1, 1, 2, 2, 2])
from sklearn.naive_bayes import GaussianNB
# create a classifier
clf = GaussianNB()
# Train = Fit my classifier : give it the training data and it learns the patterns
# X: features, Y: labels
clf.fit(X, Y)

# Prediction : what do you think the label is for that new point ? Which class it belongs to ?
print(clf.predict([[-0.8, -1]]))

clf_pf = GaussianNB()
clf_pf.partial_fit(X, Y, np.unique(Y))

print(clf_pf.predict([[-0.8, -1]]))
```

Typically, use 90% for train, 10% to test

## Bayes Rules
Cancer example : Reminder of the question: the prior probability of cancer is 1%, and a sensitivity and specitivity are 90%, what's the probability that someone with a positive cancer test actually has the disease?

* Prior Probability of Having Cancer P(C)=1% => P(non C) = 99%
* Sensitivity = 90% (test is positive and you have C) = P(Test Pos | C)
* Specificity = 90% (test is negative and you don't have C) = P(Test Neg | non C)
                10% (test is positive and you don't have C) = P(Test Pos | non C)    
* Posterior Probability P(C , Test Pos) = P(C).P(Test Pos | C) = 0.01*0.9=0.009=0.9%
* Posterior Probability P(non C , Pos) = P(non C).P(Pos | non C) = 99%* = 0.99*0.1=0.099=9.9%

# Mini Projet : Machine Learning for Author ID
* https://github.com/udacity/ud120-projects.git
* Enron dataset https://www.cs.cmu.edu/~./enron/
* SciKit-Learn Doc http://scikit-learn.org/stable/documentation.html

A couple of years ago, J.K. Rowling (of Harry Potter fame) tried something interesting. She wrote a book, “The Cuckoo’s Calling,” under the name Robert Galbraith. The book received some good reviews, but no one paid much attention to it--until an anonymous tipster on Twitter said it was J.K. Rowling. The London Sunday Times enlisted two experts to compare the linguistic patterns of “Cuckoo” to Rowling’s “The Casual Vacancy,” as well as to books by several other authors. After the [results of their analysis](http://languagelog.ldc.upenn.edu/nll/?p=5315) pointed strongly toward Rowling as the author, the Times directly asked the publisher if they were the same person, and the publisher confirmed. The book exploded in popularity overnight.

We’ll do something very similar in this project. We have a set of emails, half of which were written by one person and the other half by another person at the same company . Our objective is to classify the emails as written by one person or the other based only on the text of the email. We will start with Naive Bayes in this mini-project, and then expand in later projects to other algorithms.

We will start by giving you a list of strings. Each string is the text of an email, which has undergone some basic preprocessing; we will then provide the code to split the dataset into training and testing sets. (In the next lessons you’ll learn how to do this preprocessing and splitting yourself, but for now we’ll give the code to you).

One particular feature of Naive Bayes is that it’s a good algorithm for working with text classification. When dealing with text, it’s very common to treat each unique word as a feature, and since the typical person’s vocabulary is many thousands of words, this makes for a large number of features. The relative simplicity of the algorithm and the independent features assumption of Naive Bayes make it a strong performer for classifying texts. In this mini-project, you will download and install sklearn on your computer and use Naive Bayes to classify emails by author.

## Course setup
* python -m pip install --upgrade pip
* `pip install scikit-learn` - [doc](http://scikit-learn.org/stable/install.html)
* `pip install nltk`
* Project souce code : `git clone https://github.com/udacity/ud120-projects.git`

the code base contains the starter code for all the mini-projects.

Go into the tools/ directory, and run startup.py. It will first check for the python modules, then download and unzip a large dataset that we will use heavily later.

Create and train a Naive Bayes classifier in naive_bayes/nb_author_id.py. Use it to make predictions for the test set. What is the accuracy?

When training you may see the following error: UserWarning: Duplicate scores. Result may depend on feature ordering.There are probably duplicate features, or you used a classification score for a regression task. warn("Duplicate scores. Result may depend on feature ordering.")

This is a warning that two or more words happen to have the same usage patterns in the emails--as far as the algorithm is concerned, this means that two features are the same. Some algorithms will actually break (mathematically won’t work) or give multiple different answers (depending on feature ordering) when there are duplicate features and sklearn is giving us a warning. Good information, but not something we have to worry about.

Issue with dataset size :https://discussions.udacity.com/t/enron-email-dataset-size/43716/15
Dataset: https://www.cs.cmu.edu/~./enron/enron_mail_20150507.tar.gz

from sklearn.model_selection import train_test_split
-> from sklearn.cross_validation import train_test_split

Warning with paths in Windows
- 'C:/mydir'
- 'C:\\mydir'
- r'C:\mydir'
- os.path.join(mydir, myfile)

Python3
https://github.com/jrreda/ud120-projects-python3